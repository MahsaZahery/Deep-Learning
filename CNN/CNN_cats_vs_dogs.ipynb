{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "nwtott_pGmkx",
    "outputId": "a96f8a65-3493-4e5c-bf44-a0e9f47e745b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "MI0REw8jHn2t",
    "outputId": "c32f1f77-a4d0-4d3d-f5b5-c19424d0c953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VqEqlKIVIN1o",
    "outputId": "87f68bf7-5906-4cde-bc59-6e7dcbe0f2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "#cd /content/drive/My\\ Drive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jYZTNRgUL8DX",
    "outputId": "b1abd605-1ccf-482b-e5ee-3e47377df7ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Keras\n"
     ]
    }
   ],
   "source": [
    "#cd Keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rsonxTqGGmlH",
    "outputId": "b2678a21-6168-4d4a-a20f-374a7b50a8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "! python -c \"from keras import backend; print(backend.backend())\"\n",
    "#! python -c \"import keras; print(keras.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6Z8YtOLGml4"
   },
   "source": [
    "Initializing CNN: Creating an object of class Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vh1CotREGmmE"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXOJqCPQGmmf"
   },
   "source": [
    "# Adding next layers:\n",
    "\n",
    "Four steps: \n",
    "\n",
    "    1) Convolution\n",
    "    2) Max pooling\n",
    "    3) Flattening\n",
    "    4) Full connection\n",
    "\n",
    "__Step1 - Convolution__\n",
    "- Converting the image into a table of 0 and 1 pixels\n",
    "- Applying several feature detectors on the input image\n",
    "- For each feature detector that we use we slide it all over the image, and the part of the image that matches the best with the feature detector results a very high number in a table called feature map. It contains numbers with the highest numbers when the feature detector and the input match the closest.  This is the convolutional operation. We do this with many feature detectors. So, we get as many feature maps as the feature detectors, hence we need to input the number of feature detectors. Our convolutional layer is composed of all these feature maps.\n",
    "    - The number of filters is the number of feature maps. \n",
    "    - Kernel size is a tuple containing the number of rows and columns of the feature detector window\n",
    "    - Expected format of our input images:\n",
    "    Input images are converted into 3D array if they are colored and 2D array if black & white\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "P5Ccct3tGmmn",
    "outputId": "bb354cfd-57df-47ed-d543-36222cbeaf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SD-sD60bGmmv"
   },
   "source": [
    "__Step2 - Pooling__\n",
    "\n",
    "reduce the size of the feature map:\n",
    "\n",
    "1) take a two by two sub-table that we slide over the feature map and each time we take the maximum of the four cells inside these blue squares (maxpooling). This time, we do it with stride of 2 as opposed to the previous step where we slided the feature detector window with a stride of 1. So, here, the size of feature map is divided by 2 when we apply Maxpooling. so the 5-by-5 feature map is reduced to 3-by-3.\n",
    "\n",
    "2) We apply maxpooling on each of our feature maps and then we obtain our next layer composed of all these reduce feature maps and this is called pooling layer.\n",
    "\n",
    "The reason for applying this pooling step is to reduce the number of nodes we will be using in the next steps which are the flattening step and then the full connection step. In the flatteninng step, all the cells in the pooled feature maps are flattened into one huge 1D vector. If we don't reduce the size of the feature maps, we'll get a too large vector which results in too many nodes in the fully connected layer and therefore our model would be highly computational expensive. To avoid this, we use maxpooling to reduce the complexity and the time execution, w/o losing performance. How is that? Because by taking the maximum of 2-by-2 subtables of the feature maps, we are in some way keeping the information (we are keeping track of the part of the image that contained the high numbers, corresponding to where the feature detectors detected some specific features. Hence, we don't use the spatial structure, but at the same time, we manage to reduce the time complexity and we make it less computational expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4kc7EPqGmmw"
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UC3ePB1Gmnb"
   },
   "source": [
    "__Step3 - Flattening__\n",
    "\n",
    "Taking all our pooled feature maps and put them into one single vector, which will be the input to a classic ANN with fully connected layers. \n",
    "What is the need to do feature detection and maxpooling? Why not just go with flattening from the beginning? Well, that is because applying flattening from the start will result to have information only for the cell itself, regardless of the neighbor cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOtKIcGGGmni"
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSjKQbq1Gmnl"
   },
   "source": [
    "__Step4 - Fully connected ANN__\n",
    "\n",
    "A classic ANN can be a great classifier for nonlinear problems. We need to add a hidden layer and the output layer which is binary in this case (cat or dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ3FfftGGmnr"
   },
   "outputs": [],
   "source": [
    "# Using Dense to create fully connected layer\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Em1h88eCGmnx",
    "outputId": "7113b55d-cc97-4d20-d66e-3dcd36fc81ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compiling CNN\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qIrc0coGmoB"
   },
   "source": [
    "__Preprocessing using image augmentation__\n",
    "\n",
    "Before fitting our CNN model to images, we apply a technique called \"image augmentation\" to avoid overfitting. Its a technique that allows us to enrich our training set w/o adding more images and therefore avoiding overfitting. What it actually does is that it will create many batches of the images and then it will apply on each batch some random transformations on a random selection of the images, like rotating them, shearing them, flipping them, or shifting them which results in many more diverse versions of the images, and therefore a lot more training data will be achieved, and because it is a random transformation, our model will never find the same pictures across the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fjUsKB4fGmoC",
    "outputId": "10e34945-3527-4e3e-a403-ac24c3a9ad07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.7653Epoch 1/25\n",
      "8000/8000 [==============================] - 5634s 704ms/step - loss: 0.4779 - acc: 0.7653 - val_loss: 0.5380 - val_acc: 0.7554\n",
      "Epoch 2/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8559Epoch 1/25\n",
      "8000/8000 [==============================] - 1537s 192ms/step - loss: 0.3271 - acc: 0.8559 - val_loss: 0.7042 - val_acc: 0.7413\n",
      "Epoch 3/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9158Epoch 1/25\n",
      "8000/8000 [==============================] - 1538s 192ms/step - loss: 0.2053 - acc: 0.9158 - val_loss: 0.7851 - val_acc: 0.7639\n",
      "Epoch 4/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9468Epoch 1/25\n",
      "8000/8000 [==============================] - 1581s 198ms/step - loss: 0.1356 - acc: 0.9468 - val_loss: 0.9857 - val_acc: 0.7576\n",
      "Epoch 5/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9619Epoch 1/25\n",
      "8000/8000 [==============================] - 1564s 196ms/step - loss: 0.0996 - acc: 0.9619 - val_loss: 1.1612 - val_acc: 0.7578\n",
      "Epoch 6/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9709Epoch 1/25\n",
      "8000/8000 [==============================] - 1571s 196ms/step - loss: 0.0791 - acc: 0.9709 - val_loss: 1.2736 - val_acc: 0.7394\n",
      "Epoch 7/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9760Epoch 1/25\n",
      "8000/8000 [==============================] - 1573s 197ms/step - loss: 0.0663 - acc: 0.9760 - val_loss: 1.4070 - val_acc: 0.7453\n",
      "Epoch 8/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9785Epoch 1/25\n",
      "8000/8000 [==============================] - 1555s 194ms/step - loss: 0.0591 - acc: 0.9785 - val_loss: 1.5404 - val_acc: 0.7432\n",
      "Epoch 9/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9816Epoch 1/25\n",
      "8000/8000 [==============================] - 1582s 198ms/step - loss: 0.0516 - acc: 0.9816 - val_loss: 1.5340 - val_acc: 0.7630\n",
      "Epoch 10/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9829Epoch 1/25\n",
      "8000/8000 [==============================] - 1559s 195ms/step - loss: 0.0477 - acc: 0.9829 - val_loss: 1.8326 - val_acc: 0.7510\n",
      "Epoch 11/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9845Epoch 1/25\n",
      "8000/8000 [==============================] - 1550s 194ms/step - loss: 0.0443 - acc: 0.9845 - val_loss: 1.6354 - val_acc: 0.7489\n",
      "Epoch 12/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9858Epoch 1/25\n",
      "8000/8000 [==============================] - 1637s 205ms/step - loss: 0.0398 - acc: 0.9858 - val_loss: 1.7604 - val_acc: 0.7625\n",
      "Epoch 13/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9868Epoch 1/25\n",
      "8000/8000 [==============================] - 1639s 205ms/step - loss: 0.0379 - acc: 0.9868 - val_loss: 1.8257 - val_acc: 0.7629\n",
      "Epoch 14/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9879Epoch 1/25\n",
      "8000/8000 [==============================] - 1642s 205ms/step - loss: 0.0348 - acc: 0.9879 - val_loss: 1.9174 - val_acc: 0.7524\n",
      "Epoch 15/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9883Epoch 1/25\n",
      "8000/8000 [==============================] - 1630s 204ms/step - loss: 0.0337 - acc: 0.9883 - val_loss: 1.7768 - val_acc: 0.7549\n",
      "Epoch 16/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9894Epoch 1/25\n",
      "8000/8000 [==============================] - 1577s 197ms/step - loss: 0.0307 - acc: 0.9894 - val_loss: 2.1063 - val_acc: 0.7439\n",
      "Epoch 17/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9896Epoch 1/25\n",
      "8000/8000 [==============================] - 1602s 200ms/step - loss: 0.0298 - acc: 0.9896 - val_loss: 1.9061 - val_acc: 0.7643\n",
      "Epoch 18/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9906Epoch 1/25\n",
      "8000/8000 [==============================] - 1553s 194ms/step - loss: 0.0275 - acc: 0.9906 - val_loss: 2.0688 - val_acc: 0.7484\n",
      "Epoch 19/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9908Epoch 1/25\n",
      "8000/8000 [==============================] - 1536s 192ms/step - loss: 0.0267 - acc: 0.9908 - val_loss: 2.0515 - val_acc: 0.7534\n",
      "Epoch 20/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9911Epoch 1/25\n",
      "8000/8000 [==============================] - 1551s 194ms/step - loss: 0.0259 - acc: 0.9911 - val_loss: 2.0720 - val_acc: 0.7613\n",
      "Epoch 21/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9920Epoch 1/25\n",
      "8000/8000 [==============================] - 1553s 194ms/step - loss: 0.0239 - acc: 0.9920 - val_loss: 2.0310 - val_acc: 0.7711\n",
      "Epoch 22/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9922Epoch 1/25\n",
      "8000/8000 [==============================] - 1568s 196ms/step - loss: 0.0230 - acc: 0.9922 - val_loss: 2.0616 - val_acc: 0.7705\n",
      "Epoch 23/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9927Epoch 1/25\n",
      "8000/8000 [==============================] - 1532s 192ms/step - loss: 0.0215 - acc: 0.9927 - val_loss: 2.4650 - val_acc: 0.7484\n",
      "Epoch 24/25\n",
      "7999/8000 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9930Epoch 1/25\n",
      "8000/8000 [==============================] - 1537s 192ms/step - loss: 0.0212 - acc: 0.9930 - val_loss: 2.1951 - val_acc: 0.7653\n",
      "Epoch 25/25\n",
      "6734/8000 [========================>.....] - ETA: 3:22 - loss: 0.0200 - acc: 0.9933"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # values between 0 and 1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # the size of input after which the weights will be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Fitting the CNN to images\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000, # Number of images in our training set\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000) # Number of images in our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! 99% accuracy in classifiying photos of dogs vs cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN_cats or dogs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
